# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gi4gplrysz3Kl6-fkHzlYnsB0rtX930n
"""

import os

pip install -r requirements.txt

from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.document_loaders import UnstructuredFileLoader
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQAWithSourcesChain
from huggingface_hub import notebook_login
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain import HuggingFacePipeline
from langchain.text_splitter import CharacterTextSplitter
import textwrap
import sys
import os
from transformers import BitsAndBytesConfig,AutoModelForCausalLM, AutoTokenizer

pip install pdfplumber

import pdfplumber
import re
import torch

def extract_text_from_pdf(pdf_path):
      with pdfplumber.open(pdf_path) as pdf:
              text = ""
              for page in pdf.pages:
                  text += page.extract_text()
      return text

pdf_text = extract_text_from_pdf("input.pdf")

def preprocess_text(text):
      # Remove special characters, newlines, and other non-alphanumeric characters
      text = re.sub(r'[^a-zA-Z0-9\s]', '', text)

              # Lowercasing
      text = text.lower()


      return text

preprocessed_text = preprocess_text(pdf_text)
file_path = "pdffile.txt"

# Open the file in write mode
with open(file_path, "w", encoding="utf-8") as file:
    # Write the preprocessed text to the file
        file.write(preprocessed_text)

loader =TextLoader('pdffile.txt')
documents = loader.load()
embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',model_kwargs={'device': 'cuda'})

pip install faiss-gpu

pip install bitsandbytes

text_splitter=CharacterTextSplitter(separator='\n',
                                    chunk_size=1000,
                                    chunk_overlap=200)
text_chunks=text_splitter.split_documents(documents)
vectorstore=FAISS.from_documents(text_chunks, embeddings)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

token ="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"
model_name="meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name,token=token,trust_remote_code=True, padding_side="left"
)

from transformers import AutoModelForCausalLM

!pip install -U bitsandbytes

model = AutoModelForCausalLM.from_pretrained(model_name,token=token,quantization_config=bnb_config,torch_dtype=torch.float16,trust_remote_code=True)
model.config.use_cache = False

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=3000,
    num_return_sequences=1,
    repetition_penalty=1.2,
)
llm=HuggingFacePipeline(pipeline=pipe)
qa =  RetrievalQA.from_chain_type(llm=llm, chain_type = "stuff",return_source_documents=False, retriever=vectorstore.as_retriever(k=2))

def get_answer(qa_pipeline, Question):
    # Perform question answering
    result = qa_pipeline({"query": Question}, return_only_outputs=True)

    # Extract the helpful answer from the result
    answer_pattern = r"Helpful Answer:\s(.+)"
    match = re.search(answer_pattern, result['result'])
    if match:
        helpful_answer = match.group(1)
        return helpful_answer
    else:
        return None

Question = "What is the passing critera"
answer = get_answer(qa, Question)
print(answer)

Question = "What is the grace rule"
answer = get_answer(qa, Question)
print(answer)